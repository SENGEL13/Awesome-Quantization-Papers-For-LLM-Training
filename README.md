# Awesome-Quantization-Papers-For-LLM-Training

A collection of papers on training techniques for Large Language Model, compiled for easy reference and personal study.

## Overview

- "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism", arXiv, 2024. [[paper](https://arxiv.org/abs/1909.08053)] [**`TP`**]
- "GPipe: Easy Scaling with Micro-Batch Pipeline Parallelism", arXiv, 2019. [[paper](https://arxiv.org/pdf/1811.06965)] [**`PP`**]
- "PipeDream: Fast and Efficient Pipeline Parallel DNN Training", arXiv, 2018. [[paper](https://arxiv.org/pdf/1806.03377)] [**`PP`**]

